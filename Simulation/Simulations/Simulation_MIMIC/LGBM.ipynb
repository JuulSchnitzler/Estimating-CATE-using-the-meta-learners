{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:13:57.303182Z",
     "start_time": "2024-06-07T19:13:56.177394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "9ec4323e9b848147",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:13:57.319157Z",
     "start_time": "2024-06-07T19:13:57.305117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate potential outcomes\n",
    "def potential_outcomes(X, mu0_fun, mu1_fun):\n",
    "    Y0 = []\n",
    "    Y1 = []\n",
    "    for sample in X:\n",
    "        err_0 = np.random.normal(loc=0, scale=1)\n",
    "        err_1 = np.random.normal(loc=0, scale=1)\n",
    "        Yi_0 = mu0_fun(sample) + err_0\n",
    "        Yi_1 = mu1_fun(sample) + err_1\n",
    "        Y0.append(Yi_0)\n",
    "        Y1.append(Yi_1)\n",
    "    Y0 = np.clip(np.round(Y0), 0.0, 1.0)\n",
    "    Y1 = np.clip(np.round(Y1), 0.0, 1.0)\n",
    "    return Y0, Y1"
   ],
   "id": "461d4a5790868da4",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:13:57.335151Z",
     "start_time": "2024-06-07T19:13:57.320152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate feature vectors (MIMIC-IV simulation)\n",
    "def generate_feature_vectors(N, feature_names):\n",
    "    X = np.zeros((N, len(feature_names)))\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        if feature == \"sex\":\n",
    "            X[:, i] = np.random.binomial(1, 0.5, N) \n",
    "        else:\n",
    "            X[:, i] = np.random.uniform(0, 1, N)\n",
    "    return X\n",
    "\n",
    "# Propensity score function (based on confounders)\n",
    "def propensity_score(x, con_indices, beta):\n",
    "    confounder_sum = np.dot(x[con_indices], beta)  \n",
    "    return 1 / (1 + np.exp(-confounder_sum))\n",
    "\n",
    "\n",
    "# Response functions: Uses approach of simulation 6 with other confounders added:\n",
    "mu0 = lambda x: 2*x[0] - 1*x[2] + 0.4*x[3] - x[4] - 0.1*x[8] + 0.9*x[14] + 0.4*x[17] + 0.2 * x[19] + x[20] - 1\n",
    "mu1 = lambda x: mu0(x) - 2*x[18] # Other features will also influence the outcome"
   ],
   "id": "5b23d7ee1301904a",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:13:57.351116Z",
     "start_time": "2024-06-07T19:13:57.336150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simulate data for a single experiment\n",
    "def generate_data(N, feature_names, con_indices, mu0_fun, mu1_fun, treated_frac):\n",
    "    X = generate_feature_vectors(N, feature_names)\n",
    "    beta = np.random.normal(0, 1, len(con_indices))\n",
    "    e = np.array([propensity_score(x, con_indices, beta) for x in X])\n",
    "    \n",
    "    # Adjusting e to achieve desired propensity score (e.g., 10% treated uses treated_fraction 0.1):\n",
    "    e = (treated_frac / np.mean(e)) * e\n",
    "    e = np.clip(e, 0, 1)\n",
    "    \n",
    "    Y0, Y1 = potential_outcomes(X, mu0_fun, mu1_fun)\n",
    "    T = np.random.binomial(1, e, size=N)\n",
    "    Y = T * Y1 + (1 - T) * Y0\n",
    "    return X, T, Y0, Y1, Y"
   ],
   "id": "81dde75fa95770e5",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:13:57.367116Z",
     "start_time": "2024-06-07T19:13:57.354117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_dataframe(X, T, Y0, Y1, Y, columns):    \n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    df['T'] = T\n",
    "    df['Y0'] = Y0\n",
    "    df['Y1'] = Y1\n",
    "    df['Y'] = Y\n",
    "    df['cate'] = df['Y1'] - df['Y0']\n",
    "    return df"
   ],
   "id": "124c0614d9bec3b0",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:13:57.383117Z",
     "start_time": "2024-06-07T19:13:57.369116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the MSE\n",
    "def calculate_mse(test_df):\n",
    "    return mean_squared_error(test_df['cate'], test_df['pred_cate'])"
   ],
   "id": "7d8def2082468431",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:14:00.989630Z",
     "start_time": "2024-06-07T19:13:57.385124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Simulation.X_learner_confounder import X_learner_lgbm\n",
    "from Simulation.T_learner import T_learner_lgbm\n",
    "from Simulation.S_learner import S_learner_lgbm\n",
    "\n",
    "\n",
    "# Perform multiple experiments with different sample sizes\n",
    "def perform_experiments(N, X_sim, L_sim, con_indices, mu0_fun, mu1_fun, treated_frac):\n",
    "    s_mse_list, t_mse_list, x_mse_list = [], [], []\n",
    "\n",
    "    for n in N:\n",
    "        X, T, Y0, Y1, Y = generate_data(n, X_sim, con_indices, mu0_fun, mu1_fun, treated_frac)\n",
    "        df_sim = make_dataframe(X, T, Y0, Y1, Y, X_sim)\n",
    "        train_sim, test_sim = train_test_split(df_sim, test_size=0.3, random_state=None)\n",
    "\n",
    "        # Get CATE estimates (for S-, T- and X-learner)\n",
    "        s_cate_train, s_cate_test = S_learner_lgbm(train_sim, test_sim, L_sim, 'T', 'Y')\n",
    "        t_cate_train, t_cate_test = T_learner_lgbm(train_sim, test_sim, L_sim, 'T', 'Y')\n",
    "        x_cate_train, x_cate_test = X_learner_lgbm(train_sim, test_sim, L_sim, X_sim, 'T', 'Y')\n",
    "        \n",
    "        # Calculate MSE\n",
    "        s_mse_list.append(calculate_mse(s_cate_test))\n",
    "        t_mse_list.append(calculate_mse(t_cate_test))\n",
    "        x_mse_list.append(calculate_mse(x_cate_test))\n",
    "\n",
    "    return s_mse_list, t_mse_list, x_mse_list\n",
    "\n",
    "def iterate_experiments(N, num_exp, X_sim, L_sim, con_indices, mu0_fun, mu1_fun, treated_frac):\n",
    "    s_mse_tot, t_mse_tot, x_mse_tot = [], [], []\n",
    "\n",
    "    for _ in range(num_exp):\n",
    "        s_mse, t_mse, x_mse = perform_experiments(N, X_sim, L_sim, con_indices, mu0_fun, mu1_fun, treated_frac)\n",
    "        s_mse_tot.append(s_mse)\n",
    "        t_mse_tot.append(t_mse)\n",
    "        x_mse_tot.append(x_mse)\n",
    "\n",
    "    return s_mse_tot, t_mse_tot, x_mse_tot"
   ],
   "id": "78f03babe174b02",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Tuning",
   "id": "64e95fb3f05b2bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:14:01.083912Z",
     "start_time": "2024-06-07T19:14:01.069878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simulation setup MIMIC-like data (unbalanced + confounded)\n",
    "N_list = [300, 1000, 3000, 6000, 10000]\n",
    "num_experiments = 5\n",
    "confounder_indices =  [0, 2, 3, 4, 8, 14, 15, 19, 20]\n",
    "treated_fraction = 0.10\n",
    "X = [\"age\", \"sex\", \"weight\", \"height\", \"pf_ratio\", \"po2\", \"pco2\", \"ph\", \"driving_pressure\", \"lung_compliance\", \"map\", \"bilirubin\", \"creatinine\", \"platelets\", \"urea\", \"fio2\", \"hco3\", \"heart_rate\", \"minute_volume\", \"peep\", \"plateau_pressure\", \"respiratory_rate\", \"syst_blood_pressure\", \"diastolic_blood_pressure\"]\n",
    "L = [\"age\", \"weight\", \"height\", \"pf_ratio\", \"driving_pressure\", \"fio2\", \"peep\", \"plateau_pressure\"] #Pretending this is our list of confounders"
   ],
   "id": "7ceb029cd6cdbb0a",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:14:11.729696Z",
     "start_time": "2024-06-07T19:14:01.085881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LGBM Regressor as model\n",
    "s_mse_total, t_mse_total, x_mse_total= iterate_experiments(N_list, num_experiments, X, L, confounder_indices, mu0, mu1, treated_fraction)\n",
    "\n",
    "s_mse_lgbm = np.mean(s_mse_total, axis=0)\n",
    "t_mse_lgbm = np.mean(t_mse_total, axis=0)\n",
    "x_mse_lgbm = np.mean(x_mse_total, axis=0)"
   ],
   "id": "e90f7e0896aa2964",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:14:12.799881Z",
     "start_time": "2024-06-07T19:14:11.731698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plotting the average MSE for different num of samples\n",
    "plt.plot(N_list, s_mse_lgbm, marker='o', label='S-learner')\n",
    "plt.plot(N_list, t_mse_lgbm, marker='o', label='T-learner')\n",
    "plt.plot(N_list, x_mse_lgbm, marker='o', label='X-learner')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MIMIC Simulation: LGBM Regressor')\n",
    "plt.legend()\n",
    "plt.savefig(f\"30_5_2_20.png\")\n",
    "plt.show()"
   ],
   "id": "cb26c42147b0c0a3",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:14:12.815462Z",
     "start_time": "2024-06-07T19:14:12.801455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"LGBM tuning:\")\n",
    "print(\"S-learner: \")\n",
    "print(s_mse_lgbm)\n",
    "print(\"T-learner: \")\n",
    "print(t_mse_lgbm)\n",
    "print(\"X-learner: \")\n",
    "print(x_mse_lgbm)"
   ],
   "id": "5a5089b5a01017dc",
   "execution_count": 16,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
